<!DOCTYPE html>
<html>

<head>
<meta charset="UTF-8">
<title>Lane Tracking Project</title>
<!-- Latest compiled and minified CSS -->
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">

<!-- Optional theme -->
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css" integrity="sha384-rHyoN1iRsVXV4nD0JutlnGaslCJuC7uwjduW9SVrLvRYooPp2bWYgmgJQIXwl/Sp" crossorigin="anonymous">

<style>
* {
    box-sizing: border-box;
}

.column {
    float: left;
    width: 33.33%;
    padding: 5px;
}

/* Clearfix (clear floats) */
.row::after {
    content: "";
    clear: both;
    display: table;
}

li {
	padding-top:10px;
}
</style>
</head>

<body style="padding: 10px">
    <h1>Lane Tracking in Adverse Visibility Conditions</h1>
    <h2>Derek Hancock and Niko Escanilla</h2>

    <hr>
    <div>
    	<h3>Motivation</h3>
    	<p>To better enhance driving experience and prevent road accidents, the push for autonomous vehicles is only growing. An example of such a push is the Advanced Driver Assitance System. These are a set of technologies currently implemented in many vehicles and include: adaptive cruise control, GPS/traffic warnings, automated braking, lane keep assist.</p>

    	<p>Lane detection is a key area in this surge for autonomous driving and was the topic we chose to focus on. Specifically, we were curious to see if there was a robust solution for detection in adverse visibility conditions. These are conditions where illumination or inclement weather may impede the driver from clearly seeing the lane and road. For the purpose of this project, we focused on illumination problems (e.g. shadows, sun glare) using solely camera (RGB) information.</p>

    	<p><a href="project_reports/projectProposal.pdf">Project proposal.</a></p>

    	<h3>Approaches</h3>
    		<h4>Preliminary Approach</h4>
    		<p>We first researched how basic lane detection works using RGB images and found a common, general pipeline across the different resources that works on simple images. The sequence of steps can be summarized below:</p>
    		<ul>
				<li><b>Color selection</b>. Given an RGB image as input, the goal of this step is to convert the image into a grayscale image. To do so, one may use an averaging RGB channel. Another example would be to look at a different color space. That is, an HSL (hue, saturation, lightness) or HSV (hue, saturation, value) color space. As alternative representations to RGB, depending on the input image, this may make the lanes in your image appear more obvious. In turn, this will lead to better separation between lanes and roads when converted to grayscale.</li>
				<li><b>Blurring</b>. Blur the image before searching for edges in order to reduce noise.</li>
    			<li><b>Canny edge detection</b>. This is a widely used technique that extracts structural information from images by using their gradient information. Often people use the hysterosis procedure which uses two thresholds. Gradients below the small threshold are not edges, above the higher threshold are string edges, and between are weak edges. Weak edges are kept if they are connected to a strong edge.</li>
    			<li><b>Region of interest (ROI)</b>. To further reduce the amount of data to be processed, and to avoid obvious non-lanes, only search in a specified region of the image (for example, the bottom half of the image). </li>
    			<li><b>Hough transform (HT)</b>. HT uses the edge map produced by the Canny edge detector to find lane candidates</li>
    		</ul>

			<p>Following these steps provided good results on "highway" data (i.e. images in which the road and lanes are obvious), but obviously is not robust enough to find lanes perturbed by changes in illumination (or any type of noise).</p>
			<div class="row text-center">
				<div class="col-sm-3">&nbsp;</div>
				<div class="col-sm-3">
					<figure>
						<img src="images/prelim_highway.jpg" style="width:100%">
						<figcaption>Simple pipeline works fine in easy and clear highway data</figcaption>
					</figure>
				</div>
				<div class="col-sm-3">
					<figure>
						<img src="images/prelim_shadows.jpg" style="width:100%">
						<figcaption>Unable to detect the lane covered by dark shadows</figcaption>
					</figure>
				</div>
				<div class="col-sm-3">&nbsp;</div>
			</div>
			<br>
			<p>It is difficult because the shadows create edges that make it hard to detect lane edges. If the parameters are too restrictive, then few lane edges are found. If they are too loose, the lane is covered by shadow edges. In either case, it is nearly impossible to detect the edge. Obviously we need a more robust solution that can adapt to changing levels of illumination.</p>
			<div class="row text-center">
				<div class="col-sm-2">&nbsp;</div>
				<div class="col-sm-3">
					<figure>
						<img src="images/prelim_canny.png" style="width:100%">
						<figcaption>Too restrictive parameters result in a canny image with not enough information to detect the lane</figcaption>
					</figure>
				</div>
				<div class="col-sm-3">
					<figure>
						<img src="images/prelim_canny_noise.png" style="width:100%">
						<figcaption>Loose parameters result in a canny image that is too noisy</figcaption>
					</figure>
				</div>
				<div class="col-sm-3">
					<figure>
						<img src="images/prelim_hough.png" style="width:100%">
						<figcaption>The noise makes it nearly impossible to detect the lane</figcaption>
					</figure>
					</div>
				</div>

			<p><a href="project_reports/midtermReport_CS766.pdf">Mid-term report.</a></p>

    		<h4>Main Approach</h4>
    		<p>To combat illumination changes, we followed the approach used in "Gradient-Enhancing Conversion for Illumination-Robust Lane Detection" [1]. Two key ideas were proposed: gradient-enhancing conversion and adaptive Canny edge detection. 
				We also added in lane continunity from "Vision-based lane departure detection system in urban
				traffic scenes" [2]. Additionally, we developed some other custom lane filters in order to improve results.
			</p>

    		<div class="row text-center">
				<div class="col-sm-3">&nbsp;</div>
				<div class="col-sm-6">
					<figure>
						<img src="images/pipeline_overview.png" style="width:100%">
						<figcaption>Pipeline Overview</figcaption>
					</figure>
				</div>
				<div class="col-sm-3">&nbsp;</div>
			</div>	
			<p>Below is a summary of our pipeline:</p>
    		<ul>
    			<li><b>Label Training Data</b>. First note that the only possible lane colors in our images are yellow or white. So for training, we hand labeled the first 5 images in our dataset as follows: yellow-lane vs. road and white-lane vs. road. For both scenarios, we defined a road to be any pixel that was not a lane (after removing an upper percentage of the image from the training set that is clearly not lane or road). Labeling was performed by creating image masks for yellow lanes, white lanes, and roads (see images below).</li>
    			<div class="row">
  					<div class="column">
    					<img src="images/img1.jpg" alt="img1" style="width:100%">
    					<figcaption>Original input image.</figcaption>
 					 </div>
  					<div class="column">
    					<img src="images/img1_lane_w.jpg" alt="img1_white" style="width:100%">
    					<figcaption>Mask separating white lane (in white) from road (in black).</figcaption>
  					</div>
  					<div class="column">
    					<img src="images/img1_lane_y.jpg" alt="img1_yellow" style="width:100%">
    					<figcaption>Mask separating yellow lane (in yellow) from road (in black).</figcaption>
  					</div>
				</div>
    			<li><b>Linear Discriminant Analysis (LDA)</b>. Given our training data, our goal is to find weight vectors (i.e. gradient-enhanced conversion vector) that, respectively, maximize the difference between the classes. In doing so, this step outputs two weight vectors. One that best separates yellow lanes from the road and one that best separates white lanes from the road. </li>
				<li><b>Gradient Enhancing Conversion</b>. Given the two weight vectors, we apply each one to the original image. This returns a white-lane gradient enhanced image and a yellow-lane gradient enhanced image.</li>
				<div class="row text-center">
					<div class="col-sm-2">&nbsp;</div>
					<div class="col-sm-4">
						<figure>
							<img src="images/white_gradient_enhanced.png" style="width:100%">
							<figcaption>White gradient enhanced grayscale image from rgb weights [.226, .070, .704] for this particular image</figcaption>
						</figure>
					</div>
					<div class="col-sm-4">
							<figure>
								<img src="images/yellow_gradient_enhanced.png" style="width:100%">
								<figcaption>Yellow gradient enhanced grayscale image from rgb weights [.512, .437, .051] for this particular image. Notice how the yellow lane is more visible</figcaption>
							</figure>
					</div>
					<div class="col-sm-2">&nbsp;</div>
				</div>
				<li><b>Adaptive Canny Edge Detection</b>. 
					<p>We use the hysterosis procedure for edge detection, which requires two thresholds. Gradients below the small threshold are not edges, above the higher threshold are string edges, and between are weak edges. Weak edges are kept if they are connected to a strong edge. Given that we know the locations of the lane pixels and their intensities, we can use that information to inform the thresholds for canny edge detection.</p>
					<MATH>th<sub>l</sub> = |w &sdot; m <sup>l</sup> - w &sdot; m <sup>r</sup>|</MATH>
					<br>
					<MATH>th<sub>s</sub> = max(|w &sdot; m <sup>l</sup> - d|, |w &sdot; m <sup>r</sup> - d|), where d is the value for which lane and road probabilities are equal</MATH>
					<div class="row text-center">
						<div class="col-sm-4">&nbsp;</div>
						<div class="col-sm-4">
							<figure>
								<img src="images/adaptive_canny_distribution.png" style="width:100%">
								<figcaption>From [1]. Shows how the thresholds relate to the distributions of either class.</figcaption>
							</figure>
						</div>
						<div class="col-sm-4">&nbsp;</div>
					</div>
					<br>
					<p>We perform this process for both the gradient-enhanced images (white and yellow), and OR their edge images together to get a final edge image for the current frame.</p>
				</li>
				<div class="row text-center">
					<div class="col-sm-4">
						<figure>
							<img src="images/canny_white.png" style="width:100%">
							<figcaption>Canny image for white</figcaption>
						</figure>
					</div>
					<div class="col-sm-4">
							<figure>
								<img src="images/canny_yellow.png" style="width:100%">
								<figcaption>Canny image for yellow</figcaption>
							</figure>
					</div>
					<div class="col-sm-4">
						<figure>
							<img src="images/canny_combined.png" style="width:100%">
							<figcaption>Combined canny image</figcaption>
						</figure>
					</div>
				</div>
				<li><b>Hough Transform</b>. Once we have the edges in the image, we can perform the hough transform to find possible lane lines in the image. Given that we know the lanes will be on the bottom part of the image, we can exclude the edges occuring in the top part of the image (we exclude the top 40% of the image).</li>
				<div class="row text-center">
					<div class="col-sm-2">&nbsp;</div>
					<div class="col-sm-4">
						<figure>
							<img src="images/canny_combined_half.png" style="width:100%">
							<figcaption>Use the lower part of the image for finding the lane lines</figcaption>
						</figure>
					</div>
					<div class="col-sm-4">
							<figure>
								<img src="images/hough.png" style="width:100%">
								<figcaption>The resulting hough lines</figcaption>
							</figure>
					</div>
					<div class="col-sm-2">&nbsp;</div>
				</div>
				<li><b>Lane Continuity and Other Filters</b>. Ideally we will have exactly one hough line for each lane. Sometimes when we have multiple hough lines (as we do in this example), we need to filter them down to just 2 predicted hough lines.
					<ol>
						<li>Lane Continuity: [2] proposed using lane continuity to avoid predicting traffic markings in urban environments as lanes. The 
						method works well in the case of illumination variation and shadows as well, since the shadows are like markings on the road that create edges.
						The idea is simple: a hough lane prediction must pass through a "near" region (a bottom portion of the image) and a "far" region (the upper portion of the image). If it
						does not, then it is very likely not a lane line and can be removed as a prediction. We defined the near region as the bottom 30% of the image and the far region as the upper 70% of the image.
						</li>

						<li>Custom Lane Filters: We sometimes need additional lane filters in order reduce the number of hough lines further (in this example shown, lane continuity does not remove any of the lines). We introduce several other filters that effectively reduce the number of hough lines:
							<ul>
								<li>Lines with very small slopes (nearly horizontal lines) are unlikely to be lanes, so we can filter those out.</li>
								<li>We divide the remaining lines into those of positive and negative slope and keep one from each group. Lanes tend to start near the bottom of the image, so we keep the line from each group that starts closer to the bottom of the image.</li>
							</ul>
						</li>
					</ol>
				</li>
				<div class="row text-center">
					<div class="col-sm-4">&nbsp;</div>
					<div class="col-sm-4">
						<figure>
							<img src="images/hough_filtered.png" style="width:100%">
							<figcaption>Filter down to just two prediction hough lines</figcaption>
						</figure>
					</div>
					<div class="col-sm-4">&nbsp;</div>
				</div>
				<li><b>Initial Region of Interest</b>. Once we have two hough lines, we create a "region of interest", or mask, for which to extract lane edges. We set a width around the hough lines, and a distance along the hough line, for which to extract edges that are believed to be lane edges.</li>
				<div class="row text-center">
					<div class="col-sm-2">&nbsp;</div>
					<div class="col-sm-4">
						<figure>
							<img src="images/roi1.png" style="width:100%">
							<figcaption>Region of Interest 1</figcaption>
						</figure>
					</div>
					<div class="col-sm-4">
							<figure>
								<img src="images/roi2.png" style="width:100%">
								<figcaption>Region of Interest 2</figcaption>
							</figure>
					</div>
					<div class="col-sm-2">&nbsp;</div>
				</div>
				<li><b>Curve Fit</b>. Now that we have extracted the lane edges, we can fit a curve to these lane edges. This allows the algorithm to more effectively handle cases where the lane starts to curve.</li>
				<div class="row text-center">
					<div class="col-sm-2">&nbsp;</div>
					<div class="col-sm-4">
						<figure>
							<img src="images/line_fitting1.png" style="width:100%">
							<figcaption>The edges to fit a line to for lane 1</figcaption>
						</figure>
					</div>
					<div class="col-sm-4">
						<figure>
							<img src="images/line_fitting2.png" style="width:100%">
							<figcaption>The edges to fit a line to for lane 2</figcaption>
						</figure>
					</div>
					<div class="col-sm-2">&nbsp;</div>
				</div>
				<li><b>Lane Detected Image</b>. Now we can declare the two fitted curves as the lane predictions for this image.</li>
				<div class="row text-center">
					<div class="col-sm-3">&nbsp;</div>
					<div class="col-sm-6">
						<figure>
							<img src="images/lane_predictions.png" style="width:100%">
							<figcaption>The final predicted lanes</figcaption>
						</figure>
					</div>
					<div class="col-sm-3">&nbsp;</div>
				</div>				
				<li><b>Update Training Data</b>. We then update the training data for future iterations of the algorithm. We pop off the training data from 5 frames ago and add in the information from the current frame. Each pixel is a piece of training data, with its R,G, and B intensities, as well as its label (lane or road).</li>
				<div class="row text-center">
					<div class="col-sm-2">&nbsp;</div>
					<div class="col-sm-4">
						<figure>
							<img src="images/label1.png" style="width:100%">
							<figcaption>Labels for lane 1</figcaption>
						</figure>
					</div>
					<div class="col-sm-4">
						<figure>
							<img src="images/label2.png" style="width:100%">
							<figcaption>Labels for lane 2</figcaption>
						</figure>
					</div>
					<div class="col-sm-2">&nbsp;</div>
				</div>
    		</ul>

    	<h3>Implementation</h3>
    		<h4>Data</h4>
    		<p>The data used for our implementation was from Udacity's Open Source Self-Driving Car repository. The dataset we used contained a total of 5648 images and contained various lighting conditions (e.g., daytime, shadows). More details about the data can be found <a href="https://github.com/udacity/self-driving-car/tree/master/datasets/CH2">here</a>.</p>
    		<h4>Repository</h4>
    		<p>Our downloadable source and executable code can be found <a href="https://github.com/dereklh4/CS766_lane_tracking">here</a>.</p>

    	<h3>Results</h3>
    	<p>After running our pipeline on the Udacity dataset, we randomly sampled 500 images to compute accuracy measurements. [1] discussed how they computed "accuracy" by defining a term they referred to as <i>detection rate</i>. That is, they manually counted the number of correct detected lanes, where a correct prediction is when the predicted lane lies on the true lane markings and its curvature is in the same direction. Although this is a sound measurement, we wanted to be comprehensive in our analyses. In turn, we define true positive, false positive, and false negatives as follows:</p>

    	<p>
    		<li><b>True positive:</b> correctly predicts that the lane (i.e. predicted lane marking lies on true lane marking and curves in the same directions).</li>
    		<li><b>False positive:</b> incorrectly predicts that there is a lane (e.g. predicts the grass on the side of the road is a lane).</li>
    		<li><b>False negative:</b> incorrectly predicts that there is no lane when, indeed, there is one.</li>
    	</p>

    	<p>Note that each frame may contain one or more true lane markings. Therefore, after manually labelling the 500 frames with true positive, false positive, and/or false negative labels, our final accuracy measurements were the following:</p>

    	<p>
    		<li><b>Precision:</b> 0.8995</li>
    		<li><b>Recall:</b> 0.8192</li>
    		<li><b>F1-Score:</b> 0.8575</li>
    	</p>

    	<p>Here is a video of the results.</p>
    	<p><iframe width="600" height="480" src="https://www.youtube.com/embed/4Byf-bHOrDs"></iframe></p>

    	<h3>Discussion</h3>
		<p>There were several roadblocks we faced with this project. For example, it was, in and of itself, quite difficult to find sequential data for this task. In addition, the data that we ended up using for our implementation was not as HD as we would have liked. In terms of implementation, our pipeline at first outputted many candidate lanes and so we had to apply several of our own filters (e.g. eliminating candidate lanes that are vertical or horizontal) and another paper's proposed method of lane continuity [2]. </p>

		<p>With all that being said, we believe we obtained comparable results to [1], given that their measurement is more open to interpretation (according to [1], their average detection rate was 96.37%.). Looking ahead, it would be interesting to see if our results change dramatically if we were to add temporal information in training. With temporal information, we could perhaps add more weight to more recent previous images. Additionally, this method finds weights based on LDA and so it finds a simple linear combination of our R, G, and B features that best separates the class labels. Perhaps using a recurrent neural network with this temporal information may be more beneficial (albeit we may sacrific in runtime).</p>

		<p>Another future direction would be to test on more adverse visibility conditions. As previously mentioned, it was difficult to find sequential data in the first place. However, we would like to see how well our pipeline works in situations where there is heavy rain or snow. Lastly, in practice, we would ideally combine a slew of information for lane detection. Information such as light detection and ranging, GPS, and vehicle sensor information. </p>
		
		<h3>References</h3>
		<p>
		[1] H. Yoo, U. Yang, and K. Sohn. Gradient-enhancing conversion for illumination-robust
			lane detection. IEEE Transactions on Intelligent Transportation Systems, 14(3):1083-
			1094, Sept 2013.
		</p>
		<p>
		[2] Y. C. Leng and C. L. Chen. Vision-based lane departure detection system in urban
		traffic scenes. In 2010 11th International Conference on Control Automation Robotics
		Vision, pages 1875{1880, Dec 2010.
		</p>
    </div>
</body>

<!-- Latest compiled and minified JavaScript -->
<!-- <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script> -->

</html>
